{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "76d8847d-b662-4208-baee-62890b804405",
   "metadata": {},
   "source": [
    "# Zero Shot Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86ad5278-00b4-4305-8364-09e37568b8e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from plonk.pipe import PlonkPipeline\n",
    "from PIL import Image\n",
    "import torch\n",
    "import pandas as pd\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import math\n",
    "\n",
    "# === CONFIGURATION ===\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "data_folder = \"plonk/plonk/data/split_IJGIS\" \n",
    "test_csv = os.path.join(data_folder, \"val_80_fixed.csv\") # select 80 or 70 split\n",
    "metadata_xlsx = os.path.join(data_folder, \"all_20241120.xlsx\")\n",
    "batch_size = 128\n",
    "\n",
    "# === LOAD DATA ===\n",
    "test_df = pd.read_csv(test_csv, header=None)\n",
    "metadata_df = pd.read_excel(metadata_xlsx, engine=\"openpyxl\")\n",
    "vgi_column_index = 2\n",
    "metadata_df[\"filename\"] = metadata_df[\"ID\"].astype(str) + \".jpg\"\n",
    "\n",
    "# === INIT PIPELINE ===\n",
    "pipeline = PlonkPipeline(\"nicolas-dufour/PLONK_OSV_5M\").to(device)\n",
    "# nicolas-dufour/PLONK_OSV_5M\n",
    "# nicolas-dufour/PLONK_iNaturalist\n",
    "# nicolas-dufour/PLONK_YFCC\n",
    "# === METRICS ===\n",
    "recall_at_1 = 0\n",
    "recall_lat = 0\n",
    "recall_lon = 0\n",
    "total = 0\n",
    "\n",
    "results = []\n",
    "\n",
    "def round_coords(lat, lon):\n",
    "    return int(round(lat)), int(round(lon))\n",
    "\n",
    "def haversine(lat1, lon1, lat2, lon2):\n",
    "    \"\"\"Compute distance in km between two lat/lon pairs\"\"\"\n",
    "    R = 6371.0  # Earth radius in km\n",
    "    lat1, lon1, lat2, lon2 = map(math.radians, [lat1, lon1, lat2, lon2])\n",
    "    dlat = lat2 - lat1\n",
    "    dlon = lon2 - lon1\n",
    "    a = math.sin(dlat/2)**2 + math.cos(lat1) * math.cos(lat2) * math.sin(dlon/2)**2\n",
    "    c = 2 * math.asin(math.sqrt(a))\n",
    "    return R * c\n",
    "\n",
    "# === PREPARE TASKS ===\n",
    "tasks = []\n",
    "gt_cells = []\n",
    "gt_latlons = []\n",
    "filenames = []\n",
    "\n",
    "for idx, row in tqdm(test_df.iterrows(), total=len(test_df)):\n",
    "    vgi_file = row[vgi_column_index]\n",
    "    vgi_id = int(vgi_file.split(\".\")[0].replace(\"VGI\", \"\").replace(\"\\\\\", \"\"))\n",
    "    gt_row = metadata_df[metadata_df[\"ID\"] == vgi_id]\n",
    "\n",
    "    if gt_row.empty:\n",
    "        print(f\"No metadata for {vgi_file}\")\n",
    "        continue\n",
    "\n",
    "    gt_lat = gt_row[\"Latitude\"].values[0]\n",
    "    gt_lon = gt_row[\"Longitude\"].values[0]\n",
    "    gt_cell = round_coords(gt_lat, gt_lon)\n",
    "\n",
    "    image_path = os.path.join(data_folder, vgi_file.replace(\"\\\\\", \"/\"))\n",
    "    if not os.path.isfile(image_path):\n",
    "        print(f\"Missing image: {image_path}\")\n",
    "        continue\n",
    "\n",
    "    tasks.append(image_path)\n",
    "    gt_cells.append(gt_cell)\n",
    "    gt_latlons.append( (gt_lat, gt_lon) )\n",
    "    filenames.append(vgi_file)\n",
    "\n",
    "# === PROCESS IN BATCHES ===\n",
    "for i in tqdm(range(0, len(tasks), batch_size), desc=\"Batches\"):\n",
    "    batch_images = []\n",
    "    batch_gt_cells = gt_cells[i:i+batch_size]\n",
    "    batch_gt_latlons = gt_latlons[i:i+batch_size]\n",
    "    batch_filenames = filenames[i:i+batch_size]\n",
    "\n",
    "    for path in tasks[i:i+batch_size]:\n",
    "        image = Image.open(path)\n",
    "        if image.mode != \"RGB\":\n",
    "            image = image.convert(\"RGB\")\n",
    "        batch_images.append(image)\n",
    "\n",
    "    # Run pipeline on batch â€” use predictions\n",
    "    gps_coords_batch = pipeline(batch_images, batch_size=len(batch_images))  # list of (lat, lon)\n",
    "\n",
    "    for fname, gt_cell, gt_latlon, (pred_lat, pred_lon) in zip(batch_filenames, batch_gt_cells, batch_gt_latlons, gps_coords_batch):\n",
    "        pred_cell = round_coords(pred_lat, pred_lon)\n",
    "        total += 1\n",
    "\n",
    "        if pred_cell == gt_cell:\n",
    "            recall_at_1 += 1\n",
    "\n",
    "        if pred_cell[0] == gt_cell[0]:\n",
    "            recall_lat += 1\n",
    "\n",
    "        if pred_cell[1] == gt_cell[1]:\n",
    "            recall_lon += 1\n",
    "\n",
    "        # compute distance\n",
    "        dist_km = haversine(gt_latlon[0], gt_latlon[1], pred_lat, pred_lon)\n",
    "\n",
    "        results.append({\n",
    "            \"filename\": fname,\n",
    "            \"gt_lat\": gt_latlon[0],\n",
    "            \"gt_lon\": gt_latlon[1],\n",
    "            \"pred_lat\": pred_lat,\n",
    "            \"pred_lon\": pred_lon,\n",
    "            \"gt_cell_lat\": gt_cell[0],\n",
    "            \"gt_cell_lon\": gt_cell[1],\n",
    "            \"pred_cell_lat\": pred_cell[0],\n",
    "            \"pred_cell_lon\": pred_cell[1],\n",
    "            \"distance_km\": dist_km\n",
    "        })\n",
    "\n",
    "# === REPORT ===\n",
    "df_results = pd.DataFrame(results)\n",
    "mean_error = df_results[\"distance_km\"].mean()\n",
    "median_error = df_results[\"distance_km\"].median()\n",
    "\n",
    "print(f\"\\n=== RESULTS ===\")\n",
    "print(f\"Total images:     {total}\")\n",
    "print(f\"Recall@1:         {recall_at_1/total:.3f}\")\n",
    "print(f\"Latitude@1:       {recall_lat/total:.3f}\")\n",
    "print(f\"Longitude@1:      {recall_lon/total:.3f}\")\n",
    "print(f\"Mean distance:    {mean_error:.2f} km\")\n",
    "print(f\"Median distance:  {median_error:.2f} km\")\n",
    "\n",
    "# Save per-image results\n",
    "df_results.to_csv(\"predictions_with_distances.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6adfd401-2012-4d4b-9108-a0034107235e",
   "metadata": {},
   "source": [
    "# Fine-tuned model prediction\n",
    "\n",
    "Already includes checkpoints from retrieval to test performance before and after retrieval "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3514a9b0-7535-44fa-8264-2810322bd8cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import math\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "import torchvision.transforms as T\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import timm\n",
    "import numpy as np\n",
    "\n",
    "from plonk.pipe import PlonkPipeline\n",
    "\n",
    "# ================== CONFIG ==================\n",
    "class Config:\n",
    "    # Paths\n",
    "    data_folder = \"plonk/plonk/data/split_IJGIS\"\n",
    "    test_csv = os.path.join(data_folder, \"val_80_fixed.csv\")\n",
    "    metadata_xlsx = os.path.join(data_folder, \"all_20241120.xlsx\")\n",
    "    checkpoint_start = \"ian_weights80.pth\"\n",
    "    plonk_weights = \"iandisaster20osm\"\n",
    "    # Model\n",
    "    model = \"timm/vit_large_patch14_dinov2.lvd142m\"\n",
    "    img_size = 384\n",
    "\n",
    "    # Eval\n",
    "    batch_size = 128\n",
    "    thresholds_km = [1, 25, 50, 200, 750, 2500]\n",
    "    retrieval_threshold_km = 50.0\n",
    "    recall_topk = [1, 5, 10]\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "config = Config()\n",
    "\n",
    "# ================== MODEL ==================\n",
    "class TimmModel(nn.Module):\n",
    "    def __init__(self, model_name, pretrained=True, img_size=383, embed_dim=1024):\n",
    "        super().__init__()\n",
    "        self.img_size = img_size\n",
    "        self.model_name = model_name\n",
    "        self.embed_dim = embed_dim\n",
    "        self.logit_scale = nn.Parameter(torch.ones([]) * np.log(1 / 0.07))\n",
    "\n",
    "        if \"vit\" in model_name.lower():\n",
    "            new_width = img_size * 2\n",
    "            new_height = round((512 / 1024) * new_width)\n",
    "            self.img_size_wide = (new_height, new_width)\n",
    "            self.model_square = timm.create_model(model_name, pretrained=pretrained, num_classes=0,\n",
    "                                                  img_size=(img_size, img_size))\n",
    "            self.model_wide   = timm.create_model(model_name, pretrained=pretrained, num_classes=0,\n",
    "                                                  img_size=self.img_size_wide)\n",
    "            self.model_uav    = timm.create_model(model_name, pretrained=pretrained, num_classes=0,\n",
    "                                                  img_size=(img_size, img_size))\n",
    "        elif \"convnext\" in model_name.lower():\n",
    "            self.model_main = timm.create_model(model_name, pretrained=pretrained, num_classes=0)\n",
    "            self.model_uav  = timm.create_model(model_name, pretrained=pretrained, num_classes=0)\n",
    "        else:\n",
    "            self.model = timm.create_model(model_name, pretrained=pretrained, num_classes=0)\n",
    "\n",
    "    def get_config(self):\n",
    "        if hasattr(self, \"model\"):\n",
    "            return timm.data.resolve_model_data_config(self.model)\n",
    "        elif hasattr(self, \"model_main\"):\n",
    "            return timm.data.resolve_model_data_config(self.model_main)\n",
    "        else:\n",
    "            return timm.data.resolve_model_data_config(self.model_square)\n",
    "\n",
    "    def forward(self, data_dict):\n",
    "        out = {}\n",
    "        for key, img in data_dict.items():\n",
    "            out[key] = self._forward_single(img, key)\n",
    "        return out\n",
    "\n",
    "    def _forward_single(self, x, key):\n",
    "        if hasattr(self, \"model\"):\n",
    "            return self.model(x)\n",
    "        if \"vit\" in self.model_name.lower():\n",
    "            if key == \"SVI\":\n",
    "                return self.model_wide(x)\n",
    "            elif key == \"UAV\":\n",
    "                return self.model_uav(x)\n",
    "            elif key in [\"RSI\", \"VGI\"]:\n",
    "                return self.model_square(x)\n",
    "            else:\n",
    "                raise ValueError(f\"Unknown view type '{key}' for ViT model.\")\n",
    "        elif \"convnext\" in self.model_name.lower():\n",
    "            if key == \"UAV\":\n",
    "                return self.model_uav(x)\n",
    "            else:\n",
    "                return self.model_main(x)\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown model type in '{self.model_name}'\")\n",
    "\n",
    "# ================== HELPERS ==================\n",
    "def haversine(lat1, lon1, lat2, lon2):\n",
    "    R = 6371.0\n",
    "    lat1, lon1, lat2, lon2 = map(math.radians, [float(lat1), float(lon1), float(lat2), float(lon2)])\n",
    "    dlat, dlon = lat2 - lat1, lon2 - lon1\n",
    "    a = math.sin(dlat/2)**2 + math.cos(lat1)*math.cos(lat2)*math.sin(dlon/2)**2\n",
    "    return 2 * R * math.asin(math.sqrt(a))\n",
    "\n",
    "def haversine_vec(lat, lon, lat_vec, lon_vec):\n",
    "    R = 6371.0\n",
    "    lat = math.radians(float(lat)); lon = math.radians(float(lon))\n",
    "    lat2 = torch.deg2rad(lat_vec.float())\n",
    "    lon2 = torch.deg2rad(lon_vec.float())\n",
    "    dlat = lat2 - lat\n",
    "    dlon = lon2 - lon\n",
    "    a = torch.sin(dlat/2)**2 + torch.cos(torch.tensor(lat)) * torch.cos(lat2) * torch.sin(dlon/2)**2\n",
    "    return 2 * R * torch.asin(torch.sqrt(a))\n",
    "\n",
    "def _id_from_relpath(rel_path: str) -> int:\n",
    "    name = os.path.basename(rel_path.replace(\"\\\\\", \"/\"))\n",
    "    return int(os.path.splitext(name)[0])\n",
    "\n",
    "def _abs_path(rel_path: str) -> str:\n",
    "    return os.path.join(config.data_folder, rel_path.replace(\"\\\\\", \"/\"))\n",
    "\n",
    "# ================== LOAD METADATA ==================\n",
    "test_df = pd.read_csv(config.test_csv, header=None)\n",
    "metadata_df = pd.read_excel(config.metadata_xlsx)\n",
    "id2lat = dict(zip(metadata_df[\"ID\"].astype(int).tolist(), metadata_df[\"Latitude\"].tolist()))\n",
    "id2lon = dict(zip(metadata_df[\"ID\"].astype(int).tolist(), metadata_df[\"Longitude\"].tolist()))\n",
    "\n",
    "# ================== LOAD MODEL ==================\n",
    "retrieval_model = TimmModel(config.model, pretrained=True, img_size=config.img_size)\n",
    "if config.checkpoint_start is not None:\n",
    "    print(\"Start from:\", config.checkpoint_start)\n",
    "    sd = torch.load(config.checkpoint_start, map_location=device)\n",
    "    retrieval_model.load_state_dict(sd, strict=False)\n",
    "retrieval_model = retrieval_model.to(device).eval()\n",
    "\n",
    "dc = retrieval_model.get_config()\n",
    "mean, std = dc[\"mean\"], dc[\"std\"]\n",
    "transform = T.Compose([\n",
    "    T.Resize((config.img_size, config.img_size)),\n",
    "    T.ToTensor(),\n",
    "    T.Normalize(mean=mean, std=std),\n",
    "])\n",
    "\n",
    "# ================== BUILD RSI EMBEDDING DB ==================\n",
    "print(\"Build RSI Embedding DB\")\n",
    "rsi_paths, rsi_ids, rsi_lats, rsi_lons = [], [], [], []\n",
    "for _, row in tqdm(test_df.iterrows(), total=len(test_df)):\n",
    "    rsi_rel = row[0]\n",
    "    rid = _id_from_relpath(rsi_rel)\n",
    "    if rid not in id2lat or rid not in id2lon: continue\n",
    "    path = _abs_path(rsi_rel)\n",
    "    if not os.path.isfile(path): continue\n",
    "    rsi_paths.append(path)\n",
    "    rsi_ids.append(rid)\n",
    "    rsi_lats.append(id2lat[rid])\n",
    "    rsi_lons.append(id2lon[rid])\n",
    "\n",
    "rsi_feats = []\n",
    "for j in tqdm(range(0, len(rsi_paths), config.batch_size), desc=\"RSI-Embeddings\"):\n",
    "    imgs = []\n",
    "    for p in rsi_paths[j:j+config.batch_size]:\n",
    "        img = Image.open(p).convert(\"RGB\")\n",
    "        imgs.append(transform(img))\n",
    "    if len(imgs) == 0: continue\n",
    "    batch = torch.stack(imgs).to(device)\n",
    "    with torch.no_grad():\n",
    "        feats = retrieval_model({\"RSI\": batch})[\"RSI\"]\n",
    "        feats = F.normalize(feats, dim=-1)\n",
    "    rsi_feats.append(feats.cpu())\n",
    "rsi_feats = torch.cat(rsi_feats, dim=0) if len(rsi_feats) else torch.empty(0, 1)\n",
    "rsi_lats_t = torch.tensor(rsi_lats)\n",
    "rsi_lons_t = torch.tensor(rsi_lons)\n",
    "rsi_ids_t  = torch.tensor(rsi_ids)\n",
    "print(f\"RSI DB size: {len(rsi_paths)}\")\n",
    "\n",
    "# ================== BUILD VGI LIST ==================\n",
    "vgi_items = []\n",
    "for _, row in test_df.iterrows():\n",
    "    vgi_rel = row[2]\n",
    "    vid = _id_from_relpath(vgi_rel)\n",
    "    path = _abs_path(vgi_rel)\n",
    "    if not os.path.isfile(path): continue\n",
    "    if vid not in id2lat or vid not in id2lon: continue\n",
    "    vgi_items.append({\n",
    "        \"id\": vid,\n",
    "        \"path\": path,\n",
    "        \"gt_lat\": id2lat[vid],\n",
    "        \"gt_lon\": id2lon[vid],\n",
    "    })\n",
    "\n",
    "# ================== EVAL PIPELINE ==================\n",
    "pipeline = PlonkPipeline(config.plonk_weights).to(device)\n",
    "\n",
    "acc_counts_before = {thr: 0 for thr in config.thresholds_km}\n",
    "acc_counts_after  = {thr: 0 for thr in config.thresholds_km}\n",
    "acc_counts_retrieval = {thr: 0 for thr in config.thresholds_km}\n",
    "\n",
    "recall_counts_before = {k: 0 for k in config.recall_topk}\n",
    "recall_counts_after  = {k: 0 for k in config.recall_topk}\n",
    "\n",
    "results_before, results_after, results_retrieval = [], [], []\n",
    "total = 0\n",
    "\n",
    "for i in tqdm(range(0, len(vgi_items), config.batch_size), desc=\"Batches\"):\n",
    "    batch = vgi_items[i:i+config.batch_size]\n",
    "    batch_imgs = [Image.open(x[\"path\"]).convert(\"RGB\") for x in batch]\n",
    "    plonk_preds = pipeline(batch_imgs, batch_size=len(batch_imgs))\n",
    "\n",
    "    refined_coords = []\n",
    "    for x, (pred_lat, pred_lon), img in zip(batch, plonk_preds, batch_imgs):\n",
    "        total += 1\n",
    "        # ---- Accuracy BEFORE ----\n",
    "        dist_before = haversine(x[\"gt_lat\"], x[\"gt_lon\"], pred_lat, pred_lon)\n",
    "        within_before = {thr: (dist_before <= thr) for thr in config.thresholds_km}\n",
    "        for thr, ok in within_before.items():\n",
    "            if ok: acc_counts_before[thr] += 1\n",
    "        results_before.append({\n",
    "            \"filename\": os.path.relpath(x[\"path\"], config.data_folder),\n",
    "            \"gt_lat\": x[\"gt_lat\"], \"gt_lon\": x[\"gt_lon\"],\n",
    "            \"pred_lat\": pred_lat, \"pred_lon\": pred_lon,\n",
    "            \"distance_km\": dist_before,\n",
    "            **{f\"within_{thr}km\": within_before[thr] for thr in config.thresholds_km},\n",
    "        })\n",
    "\n",
    "        # ---- Build VGI embedding ----\n",
    "        vgi_tensor = transform(img).unsqueeze(0).to(device)\n",
    "        with torch.no_grad():\n",
    "            vgi_feat = retrieval_model({\"VGI\": vgi_tensor})[\"VGI\"]\n",
    "            vgi_feat = F.normalize(vgi_feat, dim=-1).squeeze(0).cpu()  # shape [D]\n",
    "\n",
    "        # ---- GLOBAL Retrieval (BEFORE restriction) ----\n",
    "        # Compute similarity against the full RSI DB (no geo filter)\n",
    "        if rsi_feats.numel() > 0:\n",
    "            sims_all = torch.mv(rsi_feats, vgi_feat)                 # [N_rsi]\n",
    "            sorted_all = torch.argsort(sims_all, descending=True)    # indices into full DB\n",
    "            all_ids = rsi_ids_t                                      # [N_rsi]\n",
    "            # Recall BEFORE restriction\n",
    "            for k in config.recall_topk:\n",
    "                top_ids_all = all_ids[sorted_all[:k]].tolist()\n",
    "                hit_all = (x[\"id\"] in top_ids_all)\n",
    "                recall_counts_before[k] += int(hit_all)\n",
    "        else:\n",
    "            # No RSI DB => no recall possible\n",
    "            pass\n",
    "\n",
    "        # ---- Retrieval Candidates (AFTER applies restriction) ----\n",
    "        refined_lat, refined_lon = pred_lat, pred_lon\n",
    "        dists = haversine_vec(pred_lat, pred_lon, rsi_lats_t, rsi_lons_t) if rsi_feats.numel() > 0 else None\n",
    "        cand_mask = (dists <= config.retrieval_threshold_km) if dists is not None else torch.tensor(False)\n",
    "\n",
    "        if rsi_feats.numel() > 0 and cand_mask.any():\n",
    "            cand_idx = torch.nonzero(cand_mask, as_tuple=False).squeeze(1)   # indices into full DB\n",
    "            cand_feats = rsi_feats.index_select(0, cand_idx)                 # [Nc, D]\n",
    "            sims_cand = torch.mv(cand_feats, vgi_feat)                        # [Nc]\n",
    "            sorted_cand = torch.argsort(sims_cand, descending=True)           # indices into cand subset\n",
    "            cand_ids = rsi_ids_t[cand_idx]                                    # [Nc]\n",
    "\n",
    "            # ---- Recall AFTER restriction ----\n",
    "            for k in config.recall_topk:\n",
    "                top_ids_cand = cand_ids[sorted_cand[:k]].tolist()\n",
    "                hit_cand = (x[\"id\"] in top_ids_cand)\n",
    "                recall_counts_after[k] += int(hit_cand)\n",
    "\n",
    "            # ---- Refinement (Top-1 from restricted pool) ----\n",
    "            best_local_in_cand = cand_idx[torch.argmax(sims_cand)].item()     # index into full DB\n",
    "            refined_lat = float(rsi_lats_t[best_local_in_cand])\n",
    "            refined_lon = float(rsi_lons_t[best_local_in_cand])\n",
    "\n",
    "            # ---- Accuracy RETRIEVAL (pure Top-1 from restricted pool) ----\n",
    "            dist_retrieval = haversine(\n",
    "                x[\"gt_lat\"], x[\"gt_lon\"], refined_lat, refined_lon\n",
    "            )\n",
    "            within_retrieval = {thr: (dist_retrieval <= thr) for thr in config.thresholds_km}\n",
    "            for thr, ok in within_retrieval.items():\n",
    "                if ok: acc_counts_retrieval[thr] += 1\n",
    "            results_retrieval.append({\n",
    "                \"filename\": os.path.relpath(x[\"path\"], config.data_folder),\n",
    "                \"gt_lat\": x[\"gt_lat\"], \"gt_lon\": x[\"gt_lon\"],\n",
    "                \"pred_lat\": refined_lat, \"pred_lon\": refined_lon,\n",
    "                \"distance_km\": dist_retrieval,\n",
    "                **{f\"within_{thr}km\": within_retrieval[thr] for thr in config.thresholds_km},\n",
    "            })\n",
    "        else:\n",
    "            # No candidates in radius (or empty DB): AFTER recall contributes 0; keep Plonk coords for \"after\" accuracy\n",
    "            pass\n",
    "\n",
    "        refined_coords.append((refined_lat, refined_lon))\n",
    "\n",
    "\n",
    "    # ---- Accuracy AFTER ----\n",
    "    for x, (pred_lat, pred_lon) in zip(batch, refined_coords):\n",
    "        dist_after = haversine(x[\"gt_lat\"], x[\"gt_lon\"], pred_lat, pred_lon)\n",
    "        within_after = {thr: (dist_after <= thr) for thr in config.thresholds_km}\n",
    "        for thr, ok in within_after.items():\n",
    "            if ok: acc_counts_after[thr] += 1\n",
    "        results_after.append({\n",
    "            \"filename\": os.path.relpath(x[\"path\"], config.data_folder),\n",
    "            \"gt_lat\": x[\"gt_lat\"], \"gt_lon\": x[\"gt_lon\"],\n",
    "            \"pred_lat\": pred_lat, \"pred_lon\": pred_lon,\n",
    "            \"distance_km\": dist_after,\n",
    "            **{f\"within_{thr}km\": within_after[thr] for thr in config.thresholds_km},\n",
    "        })\n",
    "\n",
    "# ================== REPORT ==================\n",
    "df_before = pd.DataFrame(results_before)\n",
    "df_after  = pd.DataFrame(results_after)\n",
    "df_retrieval = pd.DataFrame(results_retrieval)\n",
    "\n",
    "if total == 0:\n",
    "    print(\"No predictions to report.\")\n",
    "else:\n",
    "    print(\"\\n=== RESULTS BEFORE (Plonk) ===\")\n",
    "    print(f\"Total images: {total}\")\n",
    "    for thr in config.thresholds_km:\n",
    "        print(f\"Accuracy @ {thr:>4} km: {acc_counts_before[thr]/total:.3f}\")\n",
    "    print(f\"Mean distance:   {df_before['distance_km'].mean():.2f} km\")\n",
    "    print(f\"Median distance: {df_before['distance_km'].median():.2f} km\")\n",
    "\n",
    "    print(\"\\n=== RESULTS AFTER (Plonk + Refinement) ===\")\n",
    "    for thr in config.thresholds_km:\n",
    "        print(f\"Accuracy @ {thr:>4} km: {acc_counts_after[thr]/total:.3f}\")\n",
    "    print(f\"Mean distance:   {df_after['distance_km'].mean():.2f} km\")\n",
    "    print(f\"Median distance: {df_after['distance_km'].median():.2f} km\")\n",
    "\n",
    "    print(\"\\n=== RESULTS RETRIEVAL (reines Top-1 Matching) ===\")\n",
    "    for thr in config.thresholds_km:\n",
    "        print(f\"Accuracy @ {thr:>4} km: {acc_counts_retrieval[thr]/total:.3f}\")\n",
    "    print(f\"Mean distance:   {df_retrieval['distance_km'].mean():.2f} km\")\n",
    "    print(f\"Median distance: {df_retrieval['distance_km'].median():.2f} km\")\n",
    "\n",
    "    print(\"\\n=== RECALL @ K (BEFORE Refinement) ===\")\n",
    "    for k in config.recall_topk:\n",
    "        print(f\"Recall@{k}: {recall_counts_before[k]/total:.3f}\")\n",
    "\n",
    "    print(\"\\n=== RECALL @ K (AFTER Refinement) ===\")\n",
    "    for k in config.recall_topk:\n",
    "        print(f\"Recall@{k}: {recall_counts_after[k]/total:.3f}\")\n",
    "\n",
    "    # ---- Global mean distances across all samples ----\n",
    "    mean_dist_before    = df_before[\"distance_km\"].mean()\n",
    "    mean_dist_after     = df_after[\"distance_km\"].mean()\n",
    "    mean_dist_retrieval = df_retrieval[\"distance_km\"].mean()\n",
    "    print(\"\\n=== GLOBAL MEAN DISTANCES ===\")\n",
    "    print(f\"Generative only (Plonk):         {mean_dist_before:.2f} km\")\n",
    "    print(f\"Retrieval only (Top-1):          {mean_dist_retrieval:.2f} km\")\n",
    "    print(f\"Combined (Plonk + Refinement):   {mean_dist_after:.2f} km\")\n",
    "\n",
    "    df_before.to_csv(\"predictions_before.csv\", index=False)\n",
    "    df_after.to_csv(\"predictions_after.csv\", index=False)\n",
    "    df_retrieval.to_csv(\"predictions_retrieval.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06ffcb9c-5b56-41bd-8acd-7aa1f1313e32",
   "metadata": {},
   "source": [
    "# Fine-tuned model prediction with multiple thresholds\n",
    "\n",
    "Already includes checkpoints from retrieval to test performance before and after retrieval "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00dfd611-24c1-4d35-90d9-d1198542ec08",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import math\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "import torchvision.transforms as T\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import timm\n",
    "import numpy as np\n",
    "\n",
    "from plonk.pipe import PlonkPipeline\n",
    "\n",
    "# ================== CONFIG ==================\n",
    "class Config:\n",
    "    # Paths\n",
    "    data_folder = \"plonk/plonk/data/split_IJGIS\"\n",
    "    test_csv = os.path.join(data_folder, \"val_80_fixed.csv\")\n",
    "    metadata_xlsx = os.path.join(data_folder, \"all_20241120.xlsx\")\n",
    "    checkpoint_start = \"ian_weights80.pth\"\n",
    "    plonk_weights = \"iandisaster20osm\"\n",
    "    # Model\n",
    "    model = \"timm/vit_large_patch14_dinov2.lvd142m\"\n",
    "    img_size = 384\n",
    "\n",
    "    # Eval\n",
    "    batch_size = 128\n",
    "    thresholds_km = [1, 25, 50, 200, 750, 2500]\n",
    "    retrieval_threshold_km = 50.0\n",
    "    recall_topk = [1, 5, 10]\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "config = Config()\n",
    "\n",
    "# ================== MODEL ==================\n",
    "class TimmModel(nn.Module):\n",
    "    def __init__(self, model_name, pretrained=True, img_size=383, embed_dim=1024):\n",
    "        super().__init__()\n",
    "        self.img_size = img_size\n",
    "        self.model_name = model_name\n",
    "        self.embed_dim = embed_dim\n",
    "        self.logit_scale = nn.Parameter(torch.ones([]) * np.log(1 / 0.07))\n",
    "\n",
    "        if \"vit\" in model_name.lower():\n",
    "            new_width = img_size * 2\n",
    "            new_height = round((512 / 1024) * new_width)\n",
    "            self.img_size_wide = (new_height, new_width)\n",
    "            self.model_square = timm.create_model(model_name, pretrained=pretrained, num_classes=0,\n",
    "                                                  img_size=(img_size, img_size))\n",
    "            self.model_wide   = timm.create_model(model_name, pretrained=pretrained, num_classes=0,\n",
    "                                                  img_size=self.img_size_wide)\n",
    "            self.model_uav    = timm.create_model(model_name, pretrained=pretrained, num_classes=0,\n",
    "                                                  img_size=(img_size, img_size))\n",
    "        elif \"convnext\" in model_name.lower():\n",
    "            self.model_main = timm.create_model(model_name, pretrained=pretrained, num_classes=0)\n",
    "            self.model_uav  = timm.create_model(model_name, pretrained=pretrained, num_classes=0)\n",
    "        else:\n",
    "            self.model = timm.create_model(model_name, pretrained=pretrained, num_classes=0)\n",
    "\n",
    "    def get_config(self):\n",
    "        if hasattr(self, \"model\"):\n",
    "            return timm.data.resolve_model_data_config(self.model)\n",
    "        elif hasattr(self, \"model_main\"):\n",
    "            return timm.data.resolve_model_data_config(self.model_main)\n",
    "        else:\n",
    "            return timm.data.resolve_model_data_config(self.model_square)\n",
    "\n",
    "    def forward(self, data_dict):\n",
    "        out = {}\n",
    "        for key, img in data_dict.items():\n",
    "            out[key] = self._forward_single(img, key)\n",
    "        return out\n",
    "\n",
    "    def _forward_single(self, x, key):\n",
    "        if hasattr(self, \"model\"):\n",
    "            return self.model(x)\n",
    "        if \"vit\" in self.model_name.lower():\n",
    "            if key == \"SVI\":\n",
    "                return self.model_wide(x)\n",
    "            elif key == \"UAV\":\n",
    "                return self.model_uav(x)\n",
    "            elif key in [\"RSI\", \"VGI\"]:\n",
    "                return self.model_square(x)\n",
    "            else:\n",
    "                raise ValueError(f\"Unknown view type '{key}' for ViT model.\")\n",
    "        elif \"convnext\" in self.model_name.lower():\n",
    "            if key == \"UAV\":\n",
    "                return self.model_uav(x)\n",
    "            else:\n",
    "                return self.model_main(x)\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown model type in '{self.model_name}'\")\n",
    "\n",
    "# ================== HELPERS ==================\n",
    "def haversine(lat1, lon1, lat2, lon2):\n",
    "    R = 6371.0\n",
    "    lat1, lon1, lat2, lon2 = map(math.radians, [float(lat1), float(lon1), float(lat2), float(lon2)])\n",
    "    dlat, dlon = lat2 - lat1, lon2 - lon1\n",
    "    a = math.sin(dlat/2)**2 + math.cos(lat1)*math.cos(lat2)*math.sin(dlon/2)**2\n",
    "    return 2 * R * math.asin(math.sqrt(a))\n",
    "\n",
    "def haversine_vec(lat, lon, lat_vec, lon_vec):\n",
    "    R = 6371.0\n",
    "    lat = math.radians(float(lat)); lon = math.radians(float(lon))\n",
    "    lat2 = torch.deg2rad(lat_vec.float())\n",
    "    lon2 = torch.deg2rad(lon_vec.float())\n",
    "    dlat = lat2 - lat\n",
    "    dlon = lon2 - lon\n",
    "    a = torch.sin(dlat/2)**2 + torch.cos(torch.tensor(lat)) * torch.cos(lat2) * torch.sin(dlon/2)**2\n",
    "    return 2 * R * torch.asin(torch.sqrt(a))\n",
    "\n",
    "def _id_from_relpath(rel_path: str) -> int:\n",
    "    name = os.path.basename(rel_path.replace(\"\\\\\", \"/\"))\n",
    "    return int(os.path.splitext(name)[0])\n",
    "\n",
    "def _abs_path(rel_path: str) -> str:\n",
    "    return os.path.join(config.data_folder, rel_path.replace(\"\\\\\", \"/\"))\n",
    "\n",
    "# ================== LOAD METADATA ==================\n",
    "test_df = pd.read_csv(config.test_csv, header=None)\n",
    "metadata_df = pd.read_excel(config.metadata_xlsx)\n",
    "id2lat = dict(zip(metadata_df[\"ID\"].astype(int).tolist(), metadata_df[\"Latitude\"].tolist()))\n",
    "id2lon = dict(zip(metadata_df[\"ID\"].astype(int).tolist(), metadata_df[\"Longitude\"].tolist()))\n",
    "\n",
    "# ================== LOAD MODEL ==================\n",
    "retrieval_model = TimmModel(config.model, pretrained=True, img_size=config.img_size)\n",
    "if config.checkpoint_start is not None:\n",
    "    print(\"Start from:\", config.checkpoint_start)\n",
    "    sd = torch.load(config.checkpoint_start, map_location=device)\n",
    "    retrieval_model.load_state_dict(sd, strict=False)\n",
    "retrieval_model = retrieval_model.to(device).eval()\n",
    "\n",
    "dc = retrieval_model.get_config()\n",
    "mean, std = dc[\"mean\"], dc[\"std\"]\n",
    "transform = T.Compose([\n",
    "    T.Resize((config.img_size, config.img_size)),\n",
    "    T.ToTensor(),\n",
    "    T.Normalize(mean=mean, std=std),\n",
    "])\n",
    "\n",
    "# ================== BUILD RSI EMBEDDING DB ==================\n",
    "print(\"Build RSI Embedding DB\")\n",
    "rsi_paths, rsi_ids, rsi_lats, rsi_lons = [], [], [], []\n",
    "for _, row in tqdm(test_df.iterrows(), total=len(test_df)):\n",
    "    rsi_rel = row[0]\n",
    "    rid = _id_from_relpath(rsi_rel)\n",
    "    if rid not in id2lat or rid not in id2lon: continue\n",
    "    path = _abs_path(rsi_rel)\n",
    "    if not os.path.isfile(path): continue\n",
    "    rsi_paths.append(path)\n",
    "    rsi_ids.append(rid)\n",
    "    rsi_lats.append(id2lat[rid])\n",
    "    rsi_lons.append(id2lon[rid])\n",
    "\n",
    "rsi_feats = []\n",
    "for j in tqdm(range(0, len(rsi_paths), config.batch_size), desc=\"RSI-Embeddings\"):\n",
    "    imgs = []\n",
    "    for p in rsi_paths[j:j+config.batch_size]:\n",
    "        img = Image.open(p).convert(\"RGB\")\n",
    "        imgs.append(transform(img))\n",
    "    if len(imgs) == 0: continue\n",
    "    batch = torch.stack(imgs).to(device)\n",
    "    with torch.no_grad():\n",
    "        feats = retrieval_model({\"RSI\": batch})[\"RSI\"]\n",
    "        feats = F.normalize(feats, dim=-1)\n",
    "    rsi_feats.append(feats.cpu())\n",
    "rsi_feats = torch.cat(rsi_feats, dim=0) if len(rsi_feats) else torch.empty(0, 1)\n",
    "rsi_lats_t = torch.tensor(rsi_lats)\n",
    "rsi_lons_t = torch.tensor(rsi_lons)\n",
    "rsi_ids_t  = torch.tensor(rsi_ids)\n",
    "print(f\"RSI DB size: {len(rsi_paths)}\")\n",
    "\n",
    "# ================== BUILD VGI LIST ==================\n",
    "vgi_items = []\n",
    "for _, row in test_df.iterrows():\n",
    "    vgi_rel = row[2]\n",
    "    vid = _id_from_relpath(vgi_rel)\n",
    "    path = _abs_path(vgi_rel)\n",
    "    if not os.path.isfile(path): continue\n",
    "    if vid not in id2lat or vid not in id2lon: continue\n",
    "    vgi_items.append({\n",
    "        \"id\": vid,\n",
    "        \"path\": path,\n",
    "        \"gt_lat\": id2lat[vid],\n",
    "        \"gt_lon\": id2lon[vid],\n",
    "    })\n",
    "\n",
    "# ================== COLLECT VGI EMBEDDINGS + PREDICTIONS ==================\n",
    "pipeline = PlonkPipeline(config.plonk_weights).to(device)\n",
    "\n",
    "all_vgi_feats, all_gt_lats, all_gt_lons, all_pred_lats, all_pred_lons, all_ids = [], [], [], [], [], []\n",
    "\n",
    "for i in tqdm(range(0, len(vgi_items), config.batch_size), desc=\"VGI Embeddings\"):\n",
    "    batch = vgi_items[i:i+config.batch_size]\n",
    "    batch_imgs = [Image.open(x[\"path\"]).convert(\"RGB\") for x in batch]\n",
    "\n",
    "    # Plonk predictions\n",
    "    plonk_preds = pipeline(batch_imgs, batch_size=len(batch_imgs))\n",
    "\n",
    "    # VGI embeddings\n",
    "    batch_tensors = torch.stack([transform(img) for img in batch_imgs]).to(device)\n",
    "    with torch.no_grad():\n",
    "        vgi_feats = retrieval_model({\"VGI\": batch_tensors})[\"VGI\"]\n",
    "        vgi_feats = F.normalize(vgi_feats, dim=-1).cpu()\n",
    "\n",
    "    for x, (pred_lat, pred_lon), feat in zip(batch, plonk_preds, vgi_feats):\n",
    "        all_vgi_feats.append(feat)\n",
    "        all_gt_lats.append(x[\"gt_lat\"])\n",
    "        all_gt_lons.append(x[\"gt_lon\"])\n",
    "        all_pred_lats.append(pred_lat)\n",
    "        all_pred_lons.append(pred_lon)\n",
    "        all_ids.append(x[\"id\"])\n",
    "\n",
    "all_vgi_feats = torch.stack(all_vgi_feats)  # [N, D]\n",
    "all_gt_lats = torch.tensor(all_gt_lats)\n",
    "all_gt_lons = torch.tensor(all_gt_lons)\n",
    "all_pred_lats = torch.tensor(all_pred_lats)\n",
    "all_pred_lons = torch.tensor(all_pred_lons)\n",
    "all_ids = torch.tensor(all_ids)\n",
    "\n",
    "# ================== EVAL FOR MULTIPLE THRESHOLDS ==================\n",
    "thresholds = config.thresholds_km\n",
    "results_by_thr = {}\n",
    "\n",
    "for thr in thresholds:\n",
    "    acc_counts = {t: 0 for t in thresholds}\n",
    "    total = len(all_vgi_feats)\n",
    "    distances = []\n",
    "\n",
    "    for vid, vfeat, gt_lat, gt_lon, pred_lat, pred_lon in zip(\n",
    "        all_ids, all_vgi_feats, all_gt_lats, all_gt_lons, all_pred_lats, all_pred_lons\n",
    "    ):\n",
    "        # Candidates within radius thr\n",
    "        dists = haversine_vec(pred_lat.item(), pred_lon.item(), rsi_lats_t, rsi_lons_t)\n",
    "        cand_mask = (dists <= thr)\n",
    "\n",
    "        if rsi_feats.numel() > 0 and cand_mask.any():\n",
    "            cand_idx = torch.nonzero(cand_mask, as_tuple=False).squeeze(1)\n",
    "            cand_feats = rsi_feats.index_select(0, cand_idx)\n",
    "            sims = torch.mv(cand_feats, vfeat)\n",
    "            best_idx = cand_idx[torch.argmax(sims)].item()\n",
    "\n",
    "            refined_lat = float(rsi_lats_t[best_idx])\n",
    "            refined_lon = float(rsi_lons_t[best_idx])\n",
    "        else:\n",
    "            refined_lat, refined_lon = float(pred_lat), float(pred_lon)\n",
    "\n",
    "        dist = haversine(gt_lat.item(), gt_lon.item(), refined_lat, refined_lon)\n",
    "        distances.append(dist)\n",
    "        for t in thresholds:\n",
    "            if dist <= t:\n",
    "                acc_counts[t] += 1\n",
    "\n",
    "    results_by_thr[thr] = {\n",
    "        \"acc\": {t: acc_counts[t] / total for t in thresholds},\n",
    "        \"mean_dist\": np.mean(distances),\n",
    "        \"median_dist\": np.median(distances),\n",
    "    }\n",
    "\n",
    "# ================== REPORT ==================\n",
    "for thr in thresholds:\n",
    "    print(f\"\\n=== Threshold {thr} km ===\")\n",
    "    for t in thresholds:\n",
    "        print(f\"Accuracy @ {t:>4} km: {results_by_thr[thr]['acc'][t]:.3f}\")\n",
    "    print(f\"Mean distance:   {results_by_thr[thr]['mean_dist']:.2f} km\")\n",
    "    print(f\"Median distance: {results_by_thr[thr]['median_dist']:.2f} km\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
